---
math: typst
---

# Boosting

提升Boosting是一类常用的分类问题优化方法。简单来说，Boosting通过改变样本的权重组成，训练多个被称为“弱分类器”的东西，再通过线性组合得到最终的强分类器。下面主要涉及的是AdaBoost算法，然后介绍Boosting Tree。

## Boost是什么？

历史上，Kearns和Valiant首先提出了“强可学习（strongly learnable）”和“弱可学习（weakly learnable）”的概念，并指出：

> 在概率近似正确（probably approximately correct，PAC）学习的框架中，一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。
> 
> 非常有趣的是Schapire后来证明强可学习与弱可学习是等价的，也就是说，在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。

那么我们只需要给出弱学习算法，并提升为强学习算法即可。这也是Boost一词的由来。

## AdaBoost算法

对提升方法来说，有两个问题需要回答：一是**在每一轮如何改变训练数据的权值或概率分布**；二是**如何将弱分类器组合成一个强分类器**。
